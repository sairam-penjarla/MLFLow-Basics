{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../data/diabetes_dataset.csv\")\n",
    "X = df.drop(columns=['target'], axis=0)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run() as run:\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "\n",
    "    params = {\n",
    "        \"max_depth\" : 6,\n",
    "        \"max_features\" : 3\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # Train model\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    predictions = rf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(rf, \"model-1\")\n",
    "\n",
    "    # Log additional artifacts\n",
    "    # For example, a plot of predicted vs true values\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(y_test, predictions)\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predictions\")\n",
    "    plt.title(\"True vs Predictions\")\n",
    "    plot_path = \"artifacts/plot.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    \n",
    "    # Log feature importance\n",
    "    feature_importance = rf.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"importance\": feature_importance\n",
    "    }).sort_values(by=\"importance\", ascending=False)\n",
    "    importance_csv_path = \"artifacts/feature_importance.csv\"\n",
    "    importance_df.to_csv(importance_csv_path, index=False)\n",
    "    mlflow.log_artifact(importance_csv_path)\n",
    "    \n",
    "    # Optionally, log the training and test datasets\n",
    "    np.savetxt(\"artifacts/X_train.csv\", X_train, delimiter=\",\")\n",
    "    np.savetxt(\"artifacts/X_test.csv\", X_test, delimiter=\",\")\n",
    "    np.savetxt(\"artifacts/y_train.csv\", y_train, delimiter=\",\")\n",
    "    np.savetxt(\"artifacts/y_test.csv\", y_test, delimiter=\",\")\n",
    "    mlflow.log_artifact(\"artifacts/X_train.csv\")\n",
    "    mlflow.log_artifact(\"artifacts/X_test.csv\")\n",
    "    mlflow.log_artifact(\"artifacts/y_train.csv\")\n",
    "    mlflow.log_artifact(\"artifacts/y_test.csv\")\n",
    "\n",
    "    # You can also log the run ID for future reference\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Run ID: {run_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
